services:
  redis:
    image: redis:7-alpine
    container_name: redis
    ports:
      - "6379:6379"
    command: ["redis-server", "--appendonly", "no"]

  fastapi:
    build:
      context: ../backend/fastapi
      dockerfile: Dockerfile
    container_name: fastapi
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 30s
    environment:
      - REDIS_URL=redis://redis:6379/0
      - TRITON_HTTP_URL=triton:8002
    depends_on:
      - redis
    ports:
      - "8000:8000"

  inference_worker:
    build:
      context: ../backend/inference_worker
      dockerfile: Dockerfile
    container_name: inference_worker
    healthcheck:
      test: ["CMD", "python", "healthcheck.py"]
      interval: 15s
      timeout: 5s
      retries: 5
    environment:
      - REDIS_URL=redis://redis:6379/0
      - TRITON_GRPC_URL=triton:8001
      - TRITON_HTTP_URL=triton:8002
      - NVIDIA_VISIBLE_DEVICES=0
    ports:
      - "8080:8080"

  triton:
    image: nvcr.io/nvidia/tritonserver:25.07-py3
    container_name: triton
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8002/v2/health/ready"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 30s
    command: >
      tritonserver
      --model-repository=/models
      --grpc-port=8001
      --http-port=8002
      --metrics-port=8003
      --model-control-mode=NONE
      --log-verbose=1
    ports:
      - "8001:8001"
      - "8002:8002"
      - "8003:8003"
    volumes:
      - ../models:/models
    deploy:
      resources:
        reservations:
          devices:
          -  driver: nvidia
             count: all
             capabilities: [gpu]
  frontend:
    build:
      context: ../frontend
      dockerfile: Dockerfile
    container_name: frontend

volumes:
  redis-data:
