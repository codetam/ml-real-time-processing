services:
  redis:
    image: redis:7-alpine
    container_name: redis
    ports:
      - "6379:6379"
    command: ["redis-server", "--appendonly", "no"]
  fastapi:
    build:
      context: ../backend/fastapi
      dockerfile: Dockerfile
    container_name: fastapi
    environment:
      - REDIS_URL=redis://redis:6379/0
      - TRITON_HTTP_URL=triton:8002
    depends_on:
      - redis
    ports:
      - "8000:8000"
  inference_worker:
    build:
      context: ../backend/inference_worker
      dockerfile: Dockerfile
    container_name: inference_worker
    environment:
      - REDIS_URL=redis://redis:6379/0
      - TRITON_GRPC_URL=triton:8001
      - TRITON_HTTP_URL=triton:8002
      - NVIDIA_VISIBLE_DEVICES=0
    deploy:
      resources:
        reservations:
          devices:
          -  driver: nvidia
             count: all
             capabilities: [gpu]
    depends_on:
      - redis
      - triton
  triton:
    image: nvcr.io/nvidia/tritonserver:25.07-py3
    container_name: triton
    command: >
      tritonserver
      --model-repository=/models
      --grpc-port=8001
      --http-port=8002
      --metrics-port=8003
      --model-control-mode=NONE
      --log-verbose=1
    ports:
      - "8001:8001"
      - "8002:8002"
      - "8003:8003"
    volumes:
      - ../models:/models
    deploy:
      resources:
        reservations:
          devices:
          -  driver: nvidia
             count: all
             capabilities: [gpu]
  frontend:
    build:
      context: ../frontend
      dockerfile: Dockerfile
    container_name: frontend

volumes:
  redis-data:
