services:
  fastapi:
    build:
      target: prod
    image: codetam/ml-real-time-fastapi
    restart: always

  inference_worker:
    build:
      target: prod
    image: codetam/ml-real-time-inference
    restart: always
    depends_on:
      redis:
        condition: service_started
      triton:
        condition: service_healthy
        restart: true
    deploy:
      resources:
        reservations:
          devices:
          -  driver: nvidia
             count: all
             capabilities: [gpu]

  frontend:
    build:
      target: prod
    image: codetam/ml-real-time-frontend
    restart: always
    ports:
      - 3000:80
