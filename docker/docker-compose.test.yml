services:
  fastapi:
    build:
      target: prod
    image: codetam/ml-real-time-fastapi
    restart: always

  inference_worker:
    build:
      target: prod
    image: codetam/ml-real-time-inference
    restart: always
    depends_on:
      redis:
        condition: service_started
    command: sleep infinity

  frontend:
    build:
      target: prod
    image: codetam/ml-real-time-frontend
    restart: always
    ports:
      - 3000:80
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 30s