services:
  fastapi:
    build:
      target: prod
    image: codetam/ml-real-time-fastapi
    restart: always

  inference_worker:
    build:
      target: prod
    image: codetam/ml-real-time-inference
    restart: always
    depends_on:
      redis:
        condition: service_started

  frontend:
    build:
      target: prod
    image: codetam/ml-real-time-frontend
    restart: always
    ports:
      - 3000:80